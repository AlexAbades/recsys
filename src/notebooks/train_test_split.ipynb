{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys \n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('/home/alexabades/recsys')\n",
    "from src.data.Contextual import PreProcessDataNCFContextual\n",
    "datapath = \"../data/raw/frappe/\"\n",
    "df_frappe = PreProcessDataNCFContextual(datapath)\n",
    "# df_frappe.save_data(\"frappeCtxA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user',\n",
       " 'item',\n",
       " 'rating',\n",
       " 'daytime',\n",
       " 'weather',\n",
       " 'isweekend',\n",
       " 'homework',\n",
       " 'cnt']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_frappe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cnt', 'daytime_afternoon', 'daytime_evening', 'daytime_morning', 'daytime_night', 'daytime_noon', 'daytime_sunrise', 'daytime_sunset', 'weather_cloudy', 'weather_drizzle', 'weather_foggy', 'weather_rainy', 'weather_snowy', 'weather_stormy', 'weather_sunny', 'weather_unknown', 'isweekend_weekend', 'isweekend_workday', 'homework_home', 'homework_unknown', 'homework_work']\n"
     ]
    }
   ],
   "source": [
    "cols = ['cnt', 'daytime_afternoon', 'daytime_evening',\n",
    "       'daytime_morning', 'daytime_night', 'daytime_noon', 'daytime_sunrise',\n",
    "       'daytime_sunset', 'weather_cloudy', 'weather_drizzle', 'weather_foggy',\n",
    "       'weather_rainy', 'weather_snowy', 'weather_stormy', 'weather_sunny',\n",
    "       'weather_unknown', 'isweekend_weekend', 'isweekend_workday',\n",
    "       'homework_home', 'homework_unknown', 'homework_work']\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(651, 650)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_frappe.data.user.unique()), df_frappe.data.user.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1127, 1126)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_frappe.data.item.unique()), df_frappe.data.item.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1127, 1126)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_frappe.train_ratings.item.unique()), df_frappe.train_ratings.item.unique().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alexabades/recsys/src/data/processed/frappeCtxA/frappeCtxA\n"
     ]
    }
   ],
   "source": [
    "df_frappe.save_data(\"frappeCtxA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   column1  column2  column3\n",
      "0     0.00     0.00      100\n",
      "1     0.25     0.25      200\n",
      "2     0.50     0.50      300\n",
      "3     0.75     0.75      400\n",
      "4     1.00     1.00      500\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Example DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'column1': [1, 2, 3, 4, 5],\n",
    "    'column2': [10, 20, 30, 40, 50],\n",
    "    'column3': [100, 200, 300, 400, 500]\n",
    "})\n",
    "\n",
    "# Columns to normalize\n",
    "columns = ['column1', 'column2']\n",
    "\n",
    "# Initializing the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Applying the scaler to the selected columns\n",
    "df[columns] = scaler.fit_transform(df[columns])\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have some troubles achieveing a train test split for my dataset in recomendation sistems. \n",
    "If I am dealing with sparse data, where I have some items and users with low interaction. Do we have to ensure in the train test split that at least 1 item/user appears at least once in the train and test set? \n",
    "This problem may occur also with datasets with more interactions, but in order to ilustarte in a simple example, I'll create a small dataset: \n",
    "\n",
    "Example:\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'userID': [0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5],\n",
    "    'itemID': [0, 1, 0, 2, 3, 4, 1, 5, 2, 4, 3, 5],\n",
    "})\n",
    "\n",
    "If we check the num of iteraction by user and the times an item has been interacted: \n",
    "\n",
    "num_interactions_by_user = df.groupby('userID').count()\n",
    "num_interaction_by_item = df.groupby('itemID').count()\n",
    "\n",
    "num of iteraction by user\n",
    "userID     count       \n",
    "0            2\n",
    "1            2\n",
    "2            2\n",
    "3            2\n",
    "4            2\n",
    "5            2\n",
    "\n",
    "num times an item has been interacted\n",
    "itemID     count   \n",
    "0            2 <br>\n",
    "1            2\n",
    "2            2\n",
    "3            2\n",
    "4            2\n",
    "5            2\n",
    "\n",
    "We can see that some of our samples have only 2. If we wish to train test split kind of randomly we can do something like:\n",
    "for i in df.userID.unique():\n",
    "   test_idx = df.userID.random(1)\n",
    "\n",
    "The problem of this approach is that we may end up with some items only in the test set, which is not obtimal. For example \n",
    "\n",
    "Being the index of the dataframe idx = list(df.index) we could en up with:\n",
    "idx_test = [0, 2, 4, 6, 8, 10]\n",
    "idx_train = [1, 3, 5, 7, 9, 11]\n",
    "\n",
    "We can see that in this case the test set will end up like:\n",
    "\n",
    "    userID  itemID\n",
    "0        0       0\n",
    "2        1       0\n",
    "4        2       3\n",
    "6        3       1\n",
    "8        4       2\n",
    "10       5       3\n",
    "\n",
    "And the train set like:\n",
    "\n",
    "    userID  itemID\n",
    "1        0       1\n",
    "3        1       2\n",
    "5        2       4\n",
    "7        3       5\n",
    "9        4       4\n",
    "11       5       5\n",
    "\n",
    "And wheras we might have been able to add at least one user in each set we fail to add at least one item both sets.\n",
    "\n",
    "df[np.isin(df.index, idx_test)]['itemID'].unique() = [0 3 1 2]\n",
    "df[np.isin(df.index, idx_train)]['itemID'].unique() = [1 2 4 5]\n",
    "\n",
    "We fail to add itemIDs 0 and 3 in the train set and we fail to add the itemIDs 4 and 5 in the test set. I believe that missing some items in the test set won't supose that much trouble. But if we miss one items in the train set we could face the cold start problem for that specific Item, as the model would not have seen this item in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({1, 2, 4, 5}, {0, 1, 2, 3}, {0, 1, 2, 3, 4, 5}, {1, 2})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'userID': [0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5],\n",
    "    'itemID': [0, 1, 0, 2, 3, 4, 1, 5, 2, 4, 3, 5],\n",
    "})\n",
    "\n",
    "# Create empty lists to hold the indices for the train and test sets\n",
    "idx_train = []\n",
    "idx_test = []\n",
    "\n",
    "# Loop through each user to ensure they appear in both sets\n",
    "for user_id in df['userID'].unique():\n",
    "    user_interactions = df[df['userID'] == user_id]\n",
    "    \n",
    "    # Ensure at least one interaction per user goes into each set\n",
    "    train_interaction = user_interactions.sample(n=1, random_state=42)  # Sample one interaction for training\n",
    "    test_interaction = user_interactions.drop(train_interaction.index)  # Use the other for testing\n",
    "    \n",
    "    # Append indices to the appropriate lists\n",
    "    idx_train.extend(train_interaction.index.tolist())\n",
    "    idx_test.extend(test_interaction.index.tolist())\n",
    "\n",
    "# Verify that each item appears in both sets\n",
    "train_items = df.loc[idx_train, 'itemID'].unique()\n",
    "test_items = df.loc[idx_test, 'itemID'].unique()\n",
    "\n",
    "set(train_items), set(test_items), set(train_items) | set(test_items), set(train_items) & set(test_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_element_idx_given_min_interactions(data, filter_column, ratings_column, min_interactions):\n",
    "    data_groupedby_item_interaction = data.groupby(filter_column)[\n",
    "        [ratings_column]\n",
    "    ].count()\n",
    "    iDs_low_interaction = data_groupedby_item_interaction[\n",
    "        data_groupedby_item_interaction[ratings_column] <= min_interactions\n",
    "    ].index\n",
    "\n",
    "    idx_few_interactions = np.isin(data[filter_column], iDs_low_interaction)\n",
    "    return idx_few_interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_cleaning(data, user_column, item_column, ratings_column, min_interactions):\n",
    "  clean = False \n",
    "  cnt = 0 \n",
    "  data_iter = data.copy()\n",
    "  while not clean:\n",
    "    item_indx = get_element_idx_given_min_interactions(data_iter, item_column, ratings_column, min_interactions)\n",
    "    data_clean_items = data_iter[~item_indx]\n",
    "    user_indx = get_element_idx_given_min_interactions(data_clean_items, user_column, ratings_column, min_interactions)\n",
    "    data_clean_items_users = data_clean_items[~user_indx]\n",
    "    data_iter = data_clean_items_users.copy()\n",
    "    numMinInteractionsItems = data_clean_items_users.groupby(item_column)[ratings_column].count().min()\n",
    "    numMinInteractionsUsres = data_clean_items_users.groupby(user_column)[ratings_column].count().min()\n",
    "    # clean = True\n",
    "    if numMinInteractionsItems > min_interactions and numMinInteractionsUsres > min_interactions:\n",
    "      clean = True\n",
    "    if cnt == 10:\n",
    "      print(f\"Iteration {cnt}\")\n",
    "    cnt += 1\n",
    "  return data_clean_items_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non continuous sequence\n",
    "User & Item IDs are non continuous sequence because some users or items have been removed during data cleaning. <br>\n",
    "ID Mapping: To deal with the non-continuous nature of user IDs, create a mapping between the original user IDs and a new set of indices that are continuous. For instance, you could map your 951 user IDs to indices ranging from 0 to 950. This mapping would be used when feeding data into the model and when interpreting the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "train_data = df_frappe.train_ratings.copy()\n",
    "test_data = df_frappe.test_ratings.copy()\n",
    "unique_user_id = train_data['user'].unique()\n",
    "unique_item_id = train_data['item'].unique()\n",
    "is_user_id_sequence_continuous = (max(unique_user_id) - min(unique_user_id) + 1) == len(unique_user_id)\n",
    "is_item_id_sequence_continuous = (max(train_data['item'].unique()) - min(train_data['item'].unique()) + 1) == len(train_data['item'].unique())\n",
    "print(is_user_id_sequence_continuous)\n",
    "\n",
    "def _map_elemetIDs(data, column_to_map:str):\n",
    "    unique_element_ids = sorted(data[column_to_map].unique())\n",
    "    element_id_to_index = {element_id: index for index, element_id in enumerate(unique_element_ids)}\n",
    "    return element_id_to_index\n",
    "\n",
    "user_id_to_index = _map_elemetIDs(train_data, 'user')\n",
    "\n",
    "# len(item_id_to_index.keys()), max(item_id_to_index.keys())\n",
    "# train_data['user'] = train_data['user'].apply(lambda x: item_id_to_index[x])\n",
    "# is_user_id_sequence_continuous = (max(train_data['user'].unique()) - min(train_data['user'].unique()) + 1) == len(train_data['user'].unique())\n",
    "# print(is_user_id_sequence_continuous)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0\n",
       "1          1\n",
       "2          2\n",
       "3          3\n",
       "4          4\n",
       "        ... \n",
       "96198    110\n",
       "96199     37\n",
       "96200    181\n",
       "96201    449\n",
       "96202    362\n",
       "Name: user, Length: 89099, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['user'].map(user_id_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "869"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data['user'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isItemIDSequenceContinuous = max(train_data['user'].unique()) + 1 == len(train_data['user'].unique())\n",
    "isItemIDSequenceContinuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(869, 868)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data['user'].unique()), train_data['user'].unique().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index_to_item_id = {index: item_id for item_id, index in item_id_to_index.items()}\n",
    "\n",
    "\n",
    "unique_user_ids = sorted(train_data['item'].unique())\n",
    "user_id_to_index = {user_id: index for index, user_id in enumerate(unique_user_ids)}\n",
    "index_to_user_id = {index: user_id for user_id, index in user_id_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 1,\n",
       " 2: 2,\n",
       " 3: 3,\n",
       " 4: 4,\n",
       " 5: 5,\n",
       " 6: 6,\n",
       " 7: 7,\n",
       " 8: 8,\n",
       " 9: 9,\n",
       " 10: 10,\n",
       " 11: 11,\n",
       " 12: 12,\n",
       " 13: 13,\n",
       " 14: 14,\n",
       " 15: 15,\n",
       " 16: 16,\n",
       " 17: 17,\n",
       " 18: 18,\n",
       " 19: 19,\n",
       " 20: 20,\n",
       " 21: 21,\n",
       " 22: 22,\n",
       " 23: 23,\n",
       " 24: 24,\n",
       " 25: 25,\n",
       " 26: 26,\n",
       " 27: 27,\n",
       " 28: 28,\n",
       " 29: 29,\n",
       " 30: 30,\n",
       " 31: 31,\n",
       " 32: 32,\n",
       " 33: 33,\n",
       " 34: 34,\n",
       " 35: 35,\n",
       " 36: 36,\n",
       " 37: 37,\n",
       " 38: 38,\n",
       " 39: 39,\n",
       " 40: 40,\n",
       " 41: 41,\n",
       " 42: 42,\n",
       " 43: 43,\n",
       " 44: 44,\n",
       " 45: 45,\n",
       " 46: 46,\n",
       " 47: 47,\n",
       " 48: 48,\n",
       " 49: 49,\n",
       " 50: 50,\n",
       " 51: 51,\n",
       " 52: 52,\n",
       " 53: 53,\n",
       " 54: 54,\n",
       " 55: 55,\n",
       " 56: 56,\n",
       " 57: 57,\n",
       " 58: 58,\n",
       " 59: 59,\n",
       " 60: 60,\n",
       " 61: 61,\n",
       " 62: 62,\n",
       " 63: 63,\n",
       " 64: 64,\n",
       " 65: 65,\n",
       " 66: 66,\n",
       " 67: 67,\n",
       " 68: 68,\n",
       " 69: 69,\n",
       " 70: 70,\n",
       " 71: 71,\n",
       " 72: 72,\n",
       " 73: 73,\n",
       " 74: 74,\n",
       " 75: 75,\n",
       " 76: 76,\n",
       " 77: 77,\n",
       " 78: 78,\n",
       " 79: 79,\n",
       " 80: 80,\n",
       " 81: 81,\n",
       " 82: 82,\n",
       " 83: 83,\n",
       " 84: 84,\n",
       " 85: 85,\n",
       " 86: 86,\n",
       " 87: 87,\n",
       " 88: 88,\n",
       " 89: 89,\n",
       " 90: 90,\n",
       " 91: 91,\n",
       " 92: 92,\n",
       " 93: 93,\n",
       " 94: 94,\n",
       " 95: 95,\n",
       " 96: 96,\n",
       " 97: 97,\n",
       " 98: 98,\n",
       " 99: 99,\n",
       " 100: 100,\n",
       " 101: 101,\n",
       " 102: 102,\n",
       " 103: 103,\n",
       " 104: 104,\n",
       " 105: 105,\n",
       " 106: 106,\n",
       " 107: 107,\n",
       " 108: 108,\n",
       " 109: 109,\n",
       " 110: 110,\n",
       " 111: 111,\n",
       " 112: 112,\n",
       " 113: 113,\n",
       " 114: 114,\n",
       " 115: 115,\n",
       " 116: 116,\n",
       " 117: 117,\n",
       " 118: 118,\n",
       " 119: 119,\n",
       " 120: 120,\n",
       " 121: 121,\n",
       " 122: 122,\n",
       " 123: 123,\n",
       " 124: 124,\n",
       " 125: 125,\n",
       " 126: 126,\n",
       " 127: 127,\n",
       " 128: 128,\n",
       " 129: 129,\n",
       " 130: 130,\n",
       " 131: 131,\n",
       " 132: 132,\n",
       " 133: 133,\n",
       " 134: 134,\n",
       " 135: 135,\n",
       " 136: 136,\n",
       " 137: 137,\n",
       " 138: 138,\n",
       " 139: 139,\n",
       " 140: 140,\n",
       " 141: 141,\n",
       " 142: 142,\n",
       " 143: 143,\n",
       " 144: 144,\n",
       " 145: 145,\n",
       " 146: 146,\n",
       " 147: 147,\n",
       " 148: 148,\n",
       " 149: 149,\n",
       " 150: 150,\n",
       " 151: 151,\n",
       " 152: 152,\n",
       " 153: 153,\n",
       " 154: 154,\n",
       " 155: 155,\n",
       " 156: 156,\n",
       " 157: 157,\n",
       " 158: 158,\n",
       " 159: 159,\n",
       " 160: 160,\n",
       " 161: 161,\n",
       " 162: 162,\n",
       " 163: 163,\n",
       " 164: 164,\n",
       " 165: 165,\n",
       " 166: 166,\n",
       " 167: 167,\n",
       " 168: 168,\n",
       " 169: 169,\n",
       " 170: 170,\n",
       " 171: 171,\n",
       " 172: 172,\n",
       " 173: 173,\n",
       " 174: 174,\n",
       " 175: 175,\n",
       " 176: 176,\n",
       " 177: 177,\n",
       " 178: 178,\n",
       " 179: 179,\n",
       " 180: 180,\n",
       " 181: 181,\n",
       " 182: 182,\n",
       " 183: 183,\n",
       " 184: 184,\n",
       " 185: 185,\n",
       " 186: 186,\n",
       " 187: 187,\n",
       " 188: 188,\n",
       " 189: 189,\n",
       " 190: 190,\n",
       " 191: 191,\n",
       " 192: 192,\n",
       " 193: 193,\n",
       " 194: 194,\n",
       " 195: 195,\n",
       " 196: 196,\n",
       " 197: 197,\n",
       " 198: 198,\n",
       " 199: 199,\n",
       " 200: 200,\n",
       " 201: 201,\n",
       " 202: 202,\n",
       " 203: 203,\n",
       " 204: 204,\n",
       " 205: 205,\n",
       " 206: 206,\n",
       " 207: 207,\n",
       " 208: 208,\n",
       " 209: 209,\n",
       " 210: 210,\n",
       " 211: 211,\n",
       " 212: 212,\n",
       " 213: 213,\n",
       " 214: 214,\n",
       " 215: 215,\n",
       " 216: 216,\n",
       " 217: 217,\n",
       " 218: 218,\n",
       " 219: 219,\n",
       " 220: 220,\n",
       " 221: 221,\n",
       " 222: 222,\n",
       " 223: 223,\n",
       " 224: 224,\n",
       " 225: 225,\n",
       " 226: 226,\n",
       " 227: 227,\n",
       " 228: 228,\n",
       " 229: 229,\n",
       " 230: 230,\n",
       " 231: 231,\n",
       " 232: 232,\n",
       " 233: 233,\n",
       " 234: 234,\n",
       " 235: 235,\n",
       " 236: 236,\n",
       " 237: 237,\n",
       " 238: 238,\n",
       " 239: 239,\n",
       " 240: 240,\n",
       " 241: 241,\n",
       " 242: 242,\n",
       " 243: 243,\n",
       " 244: 244,\n",
       " 245: 245,\n",
       " 246: 246,\n",
       " 247: 247,\n",
       " 248: 248,\n",
       " 249: 249,\n",
       " 250: 250,\n",
       " 251: 251,\n",
       " 252: 252,\n",
       " 253: 253,\n",
       " 254: 254,\n",
       " 255: 255,\n",
       " 256: 256,\n",
       " 257: 257,\n",
       " 258: 258,\n",
       " 259: 259,\n",
       " 260: 260,\n",
       " 261: 261,\n",
       " 262: 262,\n",
       " 263: 263,\n",
       " 264: 264,\n",
       " 265: 265,\n",
       " 266: 266,\n",
       " 267: 267,\n",
       " 268: 268,\n",
       " 269: 269,\n",
       " 270: 270,\n",
       " 271: 271,\n",
       " 272: 272,\n",
       " 273: 273,\n",
       " 274: 274,\n",
       " 275: 275,\n",
       " 276: 276,\n",
       " 277: 277,\n",
       " 278: 278,\n",
       " 279: 279,\n",
       " 280: 280,\n",
       " 281: 281,\n",
       " 282: 282,\n",
       " 283: 283,\n",
       " 284: 284,\n",
       " 286: 285,\n",
       " 287: 286,\n",
       " 288: 287,\n",
       " 289: 288,\n",
       " 290: 289,\n",
       " 291: 290,\n",
       " 292: 291,\n",
       " 293: 292,\n",
       " 294: 293,\n",
       " 295: 294,\n",
       " 296: 295,\n",
       " 297: 296,\n",
       " 298: 297,\n",
       " 299: 298,\n",
       " 300: 299,\n",
       " 301: 300,\n",
       " 302: 301,\n",
       " 303: 302,\n",
       " 304: 303,\n",
       " 305: 304,\n",
       " 306: 305,\n",
       " 307: 306,\n",
       " 308: 307,\n",
       " 309: 308,\n",
       " 310: 309,\n",
       " 311: 310,\n",
       " 312: 311,\n",
       " 313: 312,\n",
       " 314: 313,\n",
       " 315: 314,\n",
       " 316: 315,\n",
       " 317: 316,\n",
       " 318: 317,\n",
       " 319: 318,\n",
       " 320: 319,\n",
       " 321: 320,\n",
       " 322: 321,\n",
       " 323: 322,\n",
       " 324: 323,\n",
       " 325: 324,\n",
       " 326: 325,\n",
       " 327: 326,\n",
       " 328: 327,\n",
       " 329: 328,\n",
       " 330: 329,\n",
       " 331: 330,\n",
       " 332: 331,\n",
       " 333: 332,\n",
       " 334: 333,\n",
       " 335: 334,\n",
       " 337: 335,\n",
       " 338: 336,\n",
       " 339: 337,\n",
       " 340: 338,\n",
       " 341: 339,\n",
       " 342: 340,\n",
       " 343: 341,\n",
       " 344: 342,\n",
       " 345: 343,\n",
       " 346: 344,\n",
       " 347: 345,\n",
       " 348: 346,\n",
       " 349: 347,\n",
       " 350: 348,\n",
       " 351: 349,\n",
       " 352: 350,\n",
       " 353: 351,\n",
       " 354: 352,\n",
       " 355: 353,\n",
       " 356: 354,\n",
       " 357: 355,\n",
       " 358: 356,\n",
       " 359: 357,\n",
       " 360: 358,\n",
       " 361: 359,\n",
       " 362: 360,\n",
       " 363: 361,\n",
       " 364: 362,\n",
       " 365: 363,\n",
       " 366: 364,\n",
       " 367: 365,\n",
       " 368: 366,\n",
       " 369: 367,\n",
       " 370: 368,\n",
       " 371: 369,\n",
       " 372: 370,\n",
       " 373: 371,\n",
       " 374: 372,\n",
       " 375: 373,\n",
       " 376: 374,\n",
       " 377: 375,\n",
       " 378: 376,\n",
       " 379: 377,\n",
       " 380: 378,\n",
       " 381: 379,\n",
       " 382: 380,\n",
       " 383: 381,\n",
       " 384: 382,\n",
       " 385: 383,\n",
       " 386: 384,\n",
       " 387: 385,\n",
       " 388: 386,\n",
       " 389: 387,\n",
       " 390: 388,\n",
       " 391: 389,\n",
       " 392: 390,\n",
       " 393: 391,\n",
       " 394: 392,\n",
       " 395: 393,\n",
       " 396: 394,\n",
       " 397: 395,\n",
       " 398: 396,\n",
       " 399: 397,\n",
       " 400: 398,\n",
       " 401: 399,\n",
       " 402: 400,\n",
       " 403: 401,\n",
       " 404: 402,\n",
       " 405: 403,\n",
       " 406: 404,\n",
       " 407: 405,\n",
       " 408: 406,\n",
       " 409: 407,\n",
       " 410: 408,\n",
       " 411: 409,\n",
       " 412: 410,\n",
       " 413: 411,\n",
       " 414: 412,\n",
       " 415: 413,\n",
       " 416: 414,\n",
       " 417: 415,\n",
       " 418: 416,\n",
       " 419: 417,\n",
       " 420: 418,\n",
       " 421: 419,\n",
       " 422: 420,\n",
       " 423: 421,\n",
       " 424: 422,\n",
       " 425: 423,\n",
       " 426: 424,\n",
       " 427: 425,\n",
       " 428: 426,\n",
       " 429: 427,\n",
       " 430: 428,\n",
       " 431: 429,\n",
       " 432: 430,\n",
       " 433: 431,\n",
       " 434: 432,\n",
       " 435: 433,\n",
       " 436: 434,\n",
       " 437: 435,\n",
       " 438: 436,\n",
       " 439: 437,\n",
       " 440: 438,\n",
       " 441: 439,\n",
       " 442: 440,\n",
       " 443: 441,\n",
       " 444: 442,\n",
       " 445: 443,\n",
       " 446: 444,\n",
       " 447: 445,\n",
       " 448: 446,\n",
       " 449: 447,\n",
       " 450: 448,\n",
       " 451: 449,\n",
       " 452: 450,\n",
       " 453: 451,\n",
       " 454: 452,\n",
       " 457: 453,\n",
       " 458: 454,\n",
       " 459: 455,\n",
       " 460: 456,\n",
       " 461: 457,\n",
       " 462: 458,\n",
       " 463: 459,\n",
       " 464: 460,\n",
       " 465: 461,\n",
       " 466: 462,\n",
       " 467: 463,\n",
       " 468: 464,\n",
       " 469: 465,\n",
       " 470: 466,\n",
       " 471: 467,\n",
       " 472: 468,\n",
       " 473: 469,\n",
       " 474: 470,\n",
       " 475: 471,\n",
       " 476: 472,\n",
       " 477: 473,\n",
       " 478: 474,\n",
       " 479: 475,\n",
       " 480: 476,\n",
       " 481: 477,\n",
       " 482: 478,\n",
       " 483: 479,\n",
       " 484: 480,\n",
       " 485: 481,\n",
       " 486: 482,\n",
       " 487: 483,\n",
       " 488: 484,\n",
       " 489: 485,\n",
       " 490: 486,\n",
       " 491: 487,\n",
       " 492: 488,\n",
       " 493: 489,\n",
       " 494: 490,\n",
       " 495: 491,\n",
       " 496: 492,\n",
       " 497: 493,\n",
       " 498: 494,\n",
       " 499: 495,\n",
       " 501: 496,\n",
       " 502: 497,\n",
       " 503: 498,\n",
       " 504: 499,\n",
       " 505: 500,\n",
       " 506: 501,\n",
       " 507: 502,\n",
       " 508: 503,\n",
       " 509: 504,\n",
       " 510: 505,\n",
       " 511: 506,\n",
       " 512: 507,\n",
       " 513: 508,\n",
       " 514: 509,\n",
       " 515: 510,\n",
       " 516: 511,\n",
       " 518: 512,\n",
       " 519: 513,\n",
       " 520: 514,\n",
       " 521: 515,\n",
       " 522: 516,\n",
       " 523: 517,\n",
       " 524: 518,\n",
       " 525: 519,\n",
       " 526: 520,\n",
       " 527: 521,\n",
       " 528: 522,\n",
       " 529: 523,\n",
       " 530: 524,\n",
       " 531: 525,\n",
       " 532: 526,\n",
       " 533: 527,\n",
       " 534: 528,\n",
       " 535: 529,\n",
       " 536: 530,\n",
       " 537: 531,\n",
       " 538: 532,\n",
       " 539: 533,\n",
       " 540: 534,\n",
       " 541: 535,\n",
       " 543: 536,\n",
       " 544: 537,\n",
       " 545: 538,\n",
       " 546: 539,\n",
       " 547: 540,\n",
       " 548: 541,\n",
       " 549: 542,\n",
       " 550: 543,\n",
       " 551: 544,\n",
       " 552: 545,\n",
       " 553: 546,\n",
       " 554: 547,\n",
       " 555: 548,\n",
       " 556: 549,\n",
       " 557: 550,\n",
       " 558: 551,\n",
       " 559: 552,\n",
       " 560: 553,\n",
       " 561: 554,\n",
       " 563: 555,\n",
       " 564: 556,\n",
       " 565: 557,\n",
       " 566: 558,\n",
       " 567: 559,\n",
       " 568: 560,\n",
       " 569: 561,\n",
       " 570: 562,\n",
       " 571: 563,\n",
       " 573: 564,\n",
       " 574: 565,\n",
       " 575: 566,\n",
       " 576: 567,\n",
       " 577: 568,\n",
       " 578: 569,\n",
       " 579: 570,\n",
       " 580: 571,\n",
       " 581: 572,\n",
       " 582: 573,\n",
       " 583: 574,\n",
       " 584: 575,\n",
       " 585: 576,\n",
       " 586: 577,\n",
       " 587: 578,\n",
       " 588: 579,\n",
       " 589: 580,\n",
       " 590: 581,\n",
       " 591: 582,\n",
       " 592: 583,\n",
       " 593: 584,\n",
       " 594: 585,\n",
       " 595: 586,\n",
       " 596: 587,\n",
       " 597: 588,\n",
       " 598: 589,\n",
       " 599: 590,\n",
       " 600: 591,\n",
       " 601: 592,\n",
       " 602: 593,\n",
       " 603: 594,\n",
       " 604: 595,\n",
       " 605: 596,\n",
       " 606: 597,\n",
       " 607: 598,\n",
       " 608: 599,\n",
       " 609: 600,\n",
       " 610: 601,\n",
       " 611: 602,\n",
       " 612: 603,\n",
       " 613: 604,\n",
       " 614: 605,\n",
       " 615: 606,\n",
       " 616: 607,\n",
       " 617: 608,\n",
       " 618: 609,\n",
       " 619: 610,\n",
       " 620: 611,\n",
       " 621: 612,\n",
       " 622: 613,\n",
       " 623: 614,\n",
       " 624: 615,\n",
       " 625: 616,\n",
       " 626: 617,\n",
       " 627: 618,\n",
       " 628: 619,\n",
       " 629: 620,\n",
       " 630: 621,\n",
       " 631: 622,\n",
       " 633: 623,\n",
       " 634: 624,\n",
       " 636: 625,\n",
       " 637: 626,\n",
       " 638: 627,\n",
       " 639: 628,\n",
       " 640: 629,\n",
       " 641: 630,\n",
       " 642: 631,\n",
       " 644: 632,\n",
       " 645: 633,\n",
       " 647: 634,\n",
       " 648: 635,\n",
       " 649: 636,\n",
       " 650: 637,\n",
       " 651: 638,\n",
       " 652: 639,\n",
       " 653: 640,\n",
       " 654: 641,\n",
       " 655: 642,\n",
       " 656: 643,\n",
       " 658: 644,\n",
       " 659: 645,\n",
       " 660: 646,\n",
       " 661: 647,\n",
       " 663: 648,\n",
       " 664: 649,\n",
       " 665: 650,\n",
       " 667: 651,\n",
       " 668: 652,\n",
       " 669: 653,\n",
       " 670: 654,\n",
       " 671: 655,\n",
       " 672: 656,\n",
       " 673: 657,\n",
       " 674: 658,\n",
       " 675: 659,\n",
       " 676: 660,\n",
       " 677: 661,\n",
       " 678: 662,\n",
       " 679: 663,\n",
       " 680: 664,\n",
       " 681: 665,\n",
       " 682: 666,\n",
       " 683: 667,\n",
       " 684: 668,\n",
       " 685: 669,\n",
       " 686: 670,\n",
       " 687: 671,\n",
       " 688: 672,\n",
       " 689: 673,\n",
       " 690: 674,\n",
       " 691: 675,\n",
       " 692: 676,\n",
       " 693: 677,\n",
       " 694: 678,\n",
       " 695: 679,\n",
       " 696: 680,\n",
       " 697: 681,\n",
       " 698: 682,\n",
       " 699: 683,\n",
       " 700: 684,\n",
       " 701: 685,\n",
       " 702: 686,\n",
       " 703: 687,\n",
       " 705: 688,\n",
       " 707: 689,\n",
       " 708: 690,\n",
       " 709: 691,\n",
       " 710: 692,\n",
       " 711: 693,\n",
       " 712: 694,\n",
       " 713: 695,\n",
       " 714: 696,\n",
       " 715: 697,\n",
       " 716: 698,\n",
       " 717: 699,\n",
       " 718: 700,\n",
       " 719: 701,\n",
       " 720: 702,\n",
       " 721: 703,\n",
       " 722: 704,\n",
       " 723: 705,\n",
       " 724: 706,\n",
       " 726: 707,\n",
       " 728: 708,\n",
       " 729: 709,\n",
       " 730: 710,\n",
       " 731: 711,\n",
       " 732: 712,\n",
       " 733: 713,\n",
       " 734: 714,\n",
       " 735: 715,\n",
       " 736: 716,\n",
       " 738: 717,\n",
       " 740: 718,\n",
       " 741: 719,\n",
       " 742: 720,\n",
       " 743: 721,\n",
       " 744: 722,\n",
       " 745: 723,\n",
       " 746: 724,\n",
       " 747: 725,\n",
       " 748: 726,\n",
       " 749: 727,\n",
       " 751: 728,\n",
       " 752: 729,\n",
       " 753: 730,\n",
       " 755: 731,\n",
       " 756: 732,\n",
       " 757: 733,\n",
       " 758: 734,\n",
       " 759: 735,\n",
       " 760: 736,\n",
       " 761: 737,\n",
       " 762: 738,\n",
       " 763: 739,\n",
       " 764: 740,\n",
       " 766: 741,\n",
       " 767: 742,\n",
       " 768: 743,\n",
       " 769: 744,\n",
       " 770: 745,\n",
       " 771: 746,\n",
       " 772: 747,\n",
       " 773: 748,\n",
       " 774: 749,\n",
       " 777: 750,\n",
       " 778: 751,\n",
       " 779: 752,\n",
       " 780: 753,\n",
       " 781: 754,\n",
       " 782: 755,\n",
       " 783: 756,\n",
       " 784: 757,\n",
       " 785: 758,\n",
       " 786: 759,\n",
       " 788: 760,\n",
       " 790: 761,\n",
       " 791: 762,\n",
       " 792: 763,\n",
       " 794: 764,\n",
       " 795: 765,\n",
       " 796: 766,\n",
       " 798: 767,\n",
       " 799: 768,\n",
       " 800: 769,\n",
       " 801: 770,\n",
       " 802: 771,\n",
       " 803: 772,\n",
       " 805: 773,\n",
       " 806: 774,\n",
       " 807: 775,\n",
       " 808: 776,\n",
       " 809: 777,\n",
       " 810: 778,\n",
       " 811: 779,\n",
       " 812: 780,\n",
       " 813: 781,\n",
       " 814: 782,\n",
       " 815: 783,\n",
       " 817: 784,\n",
       " 818: 785,\n",
       " 819: 786,\n",
       " 820: 787,\n",
       " 821: 788,\n",
       " 823: 789,\n",
       " 825: 790,\n",
       " 826: 791,\n",
       " 827: 792,\n",
       " 828: 793,\n",
       " 829: 794,\n",
       " 830: 795,\n",
       " 831: 796,\n",
       " 832: 797,\n",
       " 833: 798,\n",
       " 834: 799,\n",
       " 836: 800,\n",
       " 837: 801,\n",
       " 838: 802,\n",
       " 839: 803,\n",
       " 840: 804,\n",
       " 841: 805,\n",
       " 842: 806,\n",
       " 843: 807,\n",
       " 844: 808,\n",
       " 846: 809,\n",
       " 847: 810,\n",
       " 848: 811,\n",
       " 849: 812,\n",
       " 850: 813,\n",
       " 852: 814,\n",
       " 853: 815,\n",
       " 854: 816,\n",
       " 857: 817,\n",
       " 858: 818,\n",
       " 859: 819,\n",
       " 860: 820,\n",
       " 861: 821,\n",
       " 862: 822,\n",
       " 863: 823,\n",
       " 864: 824,\n",
       " 865: 825,\n",
       " 866: 826,\n",
       " 867: 827,\n",
       " 868: 828,\n",
       " 869: 829,\n",
       " 873: 830,\n",
       " 874: 831,\n",
       " 875: 832,\n",
       " 878: 833,\n",
       " 879: 834,\n",
       " 880: 835,\n",
       " 881: 836,\n",
       " 882: 837,\n",
       " 884: 838,\n",
       " 886: 839,\n",
       " 887: 840,\n",
       " 888: 841,\n",
       " 890: 842,\n",
       " 892: 843,\n",
       " 896: 844,\n",
       " 897: 845,\n",
       " 900: 846,\n",
       " 901: 847,\n",
       " 902: 848,\n",
       " 903: 849,\n",
       " 905: 850,\n",
       " 907: 851,\n",
       " 913: 852,\n",
       " 915: 853,\n",
       " 918: 854,\n",
       " 919: 855,\n",
       " 920: 856,\n",
       " 923: 857,\n",
       " 926: 858,\n",
       " 927: 859,\n",
       " 931: 860,\n",
       " 932: 861,\n",
       " 936: 862,\n",
       " 937: 863,\n",
       " 939: 864,\n",
       " 945: 865,\n",
       " 946: 866,\n",
       " 947: 867,\n",
       " 949: 868}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_id_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure K items in the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({\n",
    "    'userID': [0, 0, 0, 0, 0, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5],\n",
    "    'itemID': [0, 1, 5, 7, 6, 0, 1, 7, 0, 2, 7, 5, 1, 6, 0, 1, 7, 5, 0, 6, 0, 1, 5, 6, 8],\n",
    "    'rating': [4.1, 1, 5, 4.2, 4.1, 3.8, 1, 3.7, 1.3, 2, 2.7, 5, 1, 2.6, 2, 1.7, 3.7, 5, 2, 4.6, 2.0, 1, 5, 3.6, 3.8],\n",
    "})\n",
    "user_column = 'userID'\n",
    "item_column = 'itemID'\n",
    "ratings_column = 'rating'\n",
    "min_interactions = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.groupby('userID')['itemID'].count()\n",
    "df2.groupby('itemID')['userID'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'userID': [0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5],\n",
    "    'itemID': [0, 1, 0, 2, 3, 4, 1, 5, 2, 4, 3, 5],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "unique_users_id = df.userID.unique()\n",
    "test_idx = []\n",
    "train_idx = []\n",
    "for user in unique_users_id:\n",
    "  item_found = False\n",
    "  items_imteracted_by_user = list(df[df['userID'] == user]['itemID'].values)\n",
    "  while not item_found:\n",
    "    selected_item = random.choice(items_imteracted_by_user)  \n",
    "    items_imteracted_by_user.remove(selected_item)    \n",
    "    test_sample = df[(df['userID'] == user) & (df['itemID'] == selected_item)]\n",
    "    train_samples = df[(df['userID'] == user) & (df['itemID'] != selected_item)]\n",
    "    test_sample_idx = test_sample.index[0]\n",
    "    train_samples_idx = train_samples.index[0]\n",
    "    \n",
    "    if test_sample_idx not in test_idx:\n",
    "      test_idx.append(test_sample_idx)\n",
    "      item_found = True\n",
    "    if not items_imteracted_by_user:\n",
    "      print('No items remaining')\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df[np.isin(df.index, test_idx)]\n",
    "df_train = df[~np.isin(df.index, test_idx)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are iterating over the users we are ensuring that the user apears both in the train and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(np.isin(df_train['userID'].unique(), df['userID'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem comes when we check the items. Ideally the itemIds from train and test would be the same, which would mean that all items appear once in the train and test set. \n",
    "Nevertheless, if we are in a situation where an item cannot go to both, we should include it in the train. Therefore, ideally we should obtain something like:\n",
    "\n",
    "df_train.group\n",
    "\n",
    "After this, we can check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        itemID\n",
       "userID        \n",
       "0            1\n",
       "1            1\n",
       "2            1\n",
       "3            1\n",
       "4            1\n",
       "5            1"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby('userID').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False,  True])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isin([2, 3, 1], [1, 2, 4]) # if every element from the first array is in the second array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_itemID_train = df_train['itemID'].unique()\n",
    "unique_itemID_data = df['itemID'].unique()\n",
    "np.all(np.isin(unique_itemID_data, unique_itemID_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 4}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_missing_train = set(unique_itemID_data) - set(unique_itemID_train)\n",
    "items_missing_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>itemID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        userID\n",
       "itemID        \n",
       "1         True\n",
       "2         True\n",
       "3         True\n",
       "5         True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 2\n",
    "df_train.groupby('itemID').count() >= K \n",
    "df_train.groupby('userID').count() >= K "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.all(np.isin(df_train['itemID'].unique(), df_test['itemID'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0, 4, 5, 3]), array([0, 2, 3, 1, 5]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['itemID'].unique(), df_test['itemID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3, 4]\n",
    "b = set(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "a = [1, 2, 3, 4, 5, 6, 7]\n",
    "selected_item = random.choice(a)  #\n",
    "a.remove(selected_item)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count break\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "found = False\n",
    "while not found:\n",
    "  cnt += 1\n",
    "  if cnt == 4:\n",
    "    print('count break')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True, True)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the DataFrame again after reset\n",
    "df = pd.DataFrame({\n",
    "    'userID': [0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5],\n",
    "    'itemID': [0, 1, 0, 2, 3, 4, 1, 5, 2, 4, 3, 5],\n",
    "})\n",
    "\n",
    "# Setting K value\n",
    "K = 2\n",
    "\n",
    "# Step 1: Initial Filtering - Remove users and items with fewer than K+1 interactions\n",
    "# Count interactions by user and item\n",
    "user_counts = df.groupby('userID')['itemID'].count()\n",
    "item_counts = df.groupby('itemID')['userID'].count()\n",
    "\n",
    "# Filter users and items with at least K+1 interactions\n",
    "valid_users = user_counts[user_counts >= K+1].index\n",
    "valid_items = item_counts[item_counts >= K+1].index\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_df = df[df['userID'].isin(valid_users) & df['itemID'].isin(valid_items)]\n",
    "\n",
    "# Step 2: Assuming iterative cleaning has been done as part of the initial filtering for simplicity\n",
    "\n",
    "# Step 3: Allocating interactions to training and test sets\n",
    "# Initialize train and test indices\n",
    "train_idx = []\n",
    "test_idx = []\n",
    "\n",
    "for user in filtered_df['userID'].unique():\n",
    "    user_items = filtered_df[filtered_df['userID'] == user]['itemID'].unique()\n",
    "    \n",
    "    # Randomly select an item for the test set that has not been selected before\n",
    "    test_item_selected = False\n",
    "    while not test_item_selected and user_items.size > 0:\n",
    "        test_item = np.random.choice(user_items)\n",
    "        test_item_instances = filtered_df[(filtered_df['userID'] == user) & (filtered_df['itemID'] == test_item)]\n",
    "        \n",
    "        # Ensure this item has enough interactions to be split between train and test\n",
    "        if len(test_item_instances) > 1 or (test_item in valid_items and item_counts[test_item] > K):\n",
    "            test_idx.append(test_item_instances.index[0])  # Take one instance for the test set\n",
    "            test_item_selected = True\n",
    "        \n",
    "        # Update user_items to prevent re-selection of the test item\n",
    "        user_items = user_items[user_items != test_item]\n",
    "\n",
    "    # Remaining interactions go to the training set\n",
    "    remaining_indices = filtered_df[(filtered_df['userID'] == user) & (~filtered_df['itemID'].isin([test_item]))].index\n",
    "    train_idx.extend(remaining_indices.tolist())\n",
    "\n",
    "# Convert indices to DataFrame subsets\n",
    "df_train = df.loc[train_idx]\n",
    "df_test = df.loc[test_idx]\n",
    "\n",
    "# Verification\n",
    "unique_itemID_train = df_train['itemID'].unique()\n",
    "unique_itemID_test = df_test['itemID'].unique()\n",
    "unique_userID_train = df_train['userID'].unique()\n",
    "unique_userID_test = df_test['userID'].unique()\n",
    "\n",
    "# Check if all items and users from original filtered dataset appear in both sets\n",
    "all_items_in_both_sets = np.all(np.isin(unique_itemID_train, unique_itemID_test)) and np.all(np.isin(unique_itemID_test, unique_itemID_train))\n",
    "all_users_in_both_sets = np.all(np.isin(unique_userID_train, unique_userID_test)) and np.all(np.isin(unique_userID_test, unique_userID_train))\n",
    "\n",
    "# Check if the train set satisfies the K interactions constraint\n",
    "train_item_counts_satisfy_K = np.all(df_train.groupby('itemID')['userID'].count() >= K)\n",
    "train_user_counts_satisfy_K = np.all(df_train.groupby('userID')['itemID'].count() >= K)\n",
    "\n",
    "all_items_in_both_sets, all_users_in_both_sets, train_item_counts_satisfy_K, train_user_counts_satisfy_K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
