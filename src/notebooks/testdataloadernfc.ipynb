{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Created on Aug 8, 2016\n",
    "Processing datasets. \n",
    "\n",
    "@author: Xiangnan He (xiangnanhe@gmail.com)\n",
    "'''\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "\n",
    "class Dataset(object):\n",
    "    '''\n",
    "    classdocs\n",
    "    '''\n",
    "\n",
    "    def __init__(self, path):\n",
    "        '''\n",
    "        Constructor\n",
    "        '''\n",
    "        self.trainMatrix = self.load_rating_file_as_matrix(path + \".train.rating\")\n",
    "        self.testRatings = self.load_rating_file_as_list(path + \".test.rating\")\n",
    "        self.testNegatives = self.load_negative_file(path + \".test.negative\")\n",
    "        assert len(self.testRatings) == len(self.testNegatives)\n",
    "        \n",
    "        self.num_users, self.num_items = self.trainMatrix.shape\n",
    "        \n",
    "    def load_rating_file_as_list(self, filename):\n",
    "        ratingList = []\n",
    "        with open(filename, \"r\") as f:\n",
    "            line = f.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split(\"\\t\")\n",
    "                user, item = int(arr[0]), int(arr[1])\n",
    "                ratingList.append([user, item])\n",
    "                line = f.readline()\n",
    "        return ratingList\n",
    "    \n",
    "    def load_negative_file(self, filename):\n",
    "        negativeList = []\n",
    "        with open(filename, \"r\") as f:\n",
    "            line = f.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split(\"\\t\")\n",
    "                negatives = []\n",
    "                for x in arr[1: ]:\n",
    "                    negatives.append(int(x))\n",
    "                negativeList.append(negatives)\n",
    "                line = f.readline()\n",
    "        return negativeList\n",
    "    \n",
    "    def load_rating_file_as_matrix(self, filename):\n",
    "        '''\n",
    "        Read .rating file and Return dok matrix.\n",
    "        The first line of .rating file is: num_users\\t num_items\n",
    "        '''\n",
    "        # Get number of users and items\n",
    "        num_users, num_items = 0, 0\n",
    "        with open(filename, \"r\") as f:\n",
    "            line = f.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split(\"\\t\")\n",
    "                u, i = int(arr[0]), int(arr[1])\n",
    "                num_users = max(num_users, u)\n",
    "                num_items = max(num_items, i)\n",
    "                line = f.readline()\n",
    "        # Construct matrix\n",
    "        mat = sp.dok_matrix((num_users+1, num_items+1), dtype=np.float32)\n",
    "        with open(filename, \"r\") as f:\n",
    "            line = f.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split(\"\\t\")\n",
    "                user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
    "                if (rating > 0):\n",
    "                    mat[user, item] = 1.0\n",
    "                line = f.readline()    \n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rating_file_as_list(filename):\n",
    "        ratingList = []\n",
    "        with open(filename, \"r\") as f:\n",
    "            line = f.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split(\"\\t\")\n",
    "                user, item = int(arr[0]), int(arr[1])\n",
    "                ratingList.append([user, item])\n",
    "                line = f.readline()\n",
    "        return ratingList\n",
    "    \n",
    "def load_negative_file(filename):\n",
    "    negativeList = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            negatives = []\n",
    "            for x in arr[1: ]:\n",
    "                negatives.append(int(x))\n",
    "            negativeList.append(negatives)\n",
    "            line = f.readline()\n",
    "    return negativeList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_instances(train, num_negatives):\n",
    "    user_input, item_input, labels = [],[],[]\n",
    "    num_users = train.shape[0]\n",
    "    num_items = train.shape[1]\n",
    "    for (u, i) in train.keys():\n",
    "        # positive instance\n",
    "        user_input.append(u)\n",
    "        item_input.append(i)\n",
    "        labels.append(1)\n",
    "        # negative instances\n",
    "        for t in range(num_negatives):\n",
    "            j = np.random.randint(num_items)\n",
    "            while train[u, j] != 0:\n",
    "                j = np.random.randint(num_items)\n",
    "            user_input.append(u)\n",
    "            item_input.append(j)\n",
    "            labels.append(0)\n",
    "    return user_input, item_input, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/processed/ml-1m/ml-1m\"\n",
    "\n",
    "data = Dataset(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.trainMatrix\n",
    "test = data.testRatings\n",
    "neg = data.testNegatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6040, 6040, 994169)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg), len(test), len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/alexabades/recsys')\n",
    "\n",
    "from src.data.DataLoader import MovieLensDataset\n",
    "data_path = \"../data/processed/ml-1m/ml-1m\"\n",
    "data1 = MovieLensDataset()\n",
    "data1.load_processed_data(data_path)\n",
    "\n",
    "train1 = data1.trainMatrix\n",
    "test1 = data1.testRatings\n",
    "neg1 = data1.testNegatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6040, 6040, 994169)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg1), len(test1), len(train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input, item_input, labels = get_train_instances(train, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "user_input = torch.tensor(user_input, dtype=torch.long)\n",
    "item_input = torch.tensor(item_input, dtype=torch.long)\n",
    "labels = torch.tensor(labels, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "def load_config(config_path):\n",
    "    with open(config_path, \"r\") as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "\n",
    "# Load the configuration\n",
    "config = load_config(\"../config/nfc/ml-1m-1.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [64,32,16,8]\n",
    "learning_rate =  0.001\n",
    "optimizer =  \"adam\"\n",
    "batch_size =  20\n",
    "num_epochs =  1\n",
    "num_negative_instances =  4\n",
    "dropout =  0 \n",
    "# size of the matrix factorization embedings\n",
    "num_factors =  8\n",
    "num_users, num_items = (6040, 3706)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NFC(\n",
       "  (MF_Embedding_User): Embedding(6040, 8)\n",
       "  (MF_Embedding_Item): Embedding(3706, 8)\n",
       "  (MLP_Embedding_User): Embedding(6040, 32)\n",
       "  (MLP_Embedding_Item): Embedding(3706, 32)\n",
       "  (MLP_layers): ModuleList(\n",
       "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (1): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
       "  )\n",
       "  (predict_layer): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/alexabades/recsys')\n",
    "from src.models.nfc.nfc import NFC\n",
    "\n",
    "model = NFC(num_users=num_users, num_items=num_items, mf_dim=num_factors, layers=layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import Tensor, nn, optim\n",
    "\n",
    "# Assuming user_input, item_input, and labels are your full dataset tensors\n",
    "dataset = TensorDataset(user_input, item_input, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA avaliable:  True\n",
      "Number of GPUs available: 1\n"
     ]
    }
   ],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "if cuda_available:\n",
    "  num_gpus = torch.cuda.device_count()\n",
    "  print(\"CUDA avaliable: \", cuda_available)\n",
    "  print(\"Number of GPUs available:\", num_gpus)\n",
    "else:\n",
    "    print(\"CUDA not available on your machine.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU Index: 0\n",
      "GPU Properties: _CudaDeviceProperties(name='NVIDIA GeForce GTX 1650', major=7, minor=5, total_memory=4095MB, multi_processor_count=16)\n"
     ]
    }
   ],
   "source": [
    "current_gpu = torch.cuda.current_device()\n",
    "gpu_properties = torch.cuda.get_device_properties(current_gpu)\n",
    "print(\"Current GPU Index:\", current_gpu)\n",
    "print(\"GPU Properties:\", gpu_properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU # 0 :  NVIDIA GeForce GTX 1650\n",
      "Memory Allocated: 0.0 GB\n",
      "Memory Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_gpus):\n",
    "    print(\"GPU #\", i, \": \", torch.cuda.get_device_name(i))\n",
    "    print(\"Memory Allocated:\", round(torch.cuda.memory_allocated(i)/1024**3, 1), 'GB')\n",
    "    print(\"Memory Cached:   \", round(torch.cuda.memory_reserved(i)/1024**3, 2), 'GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Compute gradients based on the loss from the current batch (backpropagation).\u001b[39;00m\n\u001b[1;32m     22\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Take one optimizer step using the gradients computed in the previous step.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/recsys/venv/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/recsys/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.train()\n",
    "for epoch in range(num_epochs): \n",
    "\n",
    "  for batch in dataloader:\n",
    "    user_input = batch[0]\n",
    "    item_input = batch[1]\n",
    "    labels = batch[2]\n",
    "    labels = labels.view(-1,1)\n",
    "    print(user_input.shape)\n",
    "    # Forward pass.\n",
    "    output = model(user_input, item_input)\n",
    "    \n",
    "    # Compute loss.\n",
    "    loss = loss_fn(output, labels)\n",
    "    \n",
    "    # Clean up gradients from the model.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Compute gradients based on the loss from the current batch (backpropagation).\n",
    "    loss.backward()\n",
    "    print(loss.backward())\n",
    "    \n",
    "    # Take one optimizer step using the gradients computed in the previous step.\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Add evaluation\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assuming user_input, item_input, and labels are your full dataset tensors\n",
    "dataset = TensorDataset(user_input, item_input, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "\n",
    "for user_batch, item_batch, label_batch in dataloader:\n",
    "    print(user_batch.shape)\n",
    "    # Forward pass through your model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      mock_out = model(user_batch, item_batch)\n",
    "    \n",
    "        \n",
    "        # Calculate loss and update model parameters\n",
    "        # ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  mock_out = model(user_input, item_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Example learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.DataLoader import MovieLensDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MovieLensDataset.__init__() missing 1 required positional argument: 'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mMovieLensDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: MovieLensDataset.__init__() missing 1 required positional argument: 'path'"
     ]
    }
   ],
   "source": [
    "data = MovieLensDataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
